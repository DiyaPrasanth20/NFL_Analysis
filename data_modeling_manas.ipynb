{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df = pd.read_csv(\"data/play_by_play_2024.csv\")\n",
    "injury2024_df = pd.read_csv(\"data/injuries_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_a(name):\n",
    "    parts = name.split(\" \", 1)\n",
    "    return f\"{parts[0][0]}.{parts[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_b(name):\n",
    "    parts = name.split(\" \", 1)\n",
    "    return f\"{parts[0][0:2]}.{parts[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def merge_play_injury_dfs(play_df, injury_df):\n",
    "    # preprocessing\n",
    "    injury_df[\"date\"] = pd.to_datetime(injury_df['date_modified'])\n",
    "    play_df[\"date\"] = pd.to_datetime(play_df['game_date'])\n",
    "    injury_df['date'] = injury_df['date'].dt.tz_localize(None)\n",
    "    play_df['date'] = play_df['date'].dt.tz_localize(None)\n",
    "\n",
    "    # filtering\n",
    "    plays_with_injuries = play_df[play_df['desc'].str.contains(\"was injured\", na=False)]\n",
    "    pattern = r'(\\w+\\.(?:\\w|-|\\.|\\')+(?: \\w+)*) was injured'\n",
    "    # Extract the injured player's name from the desc column\n",
    "    injured_players = plays_with_injuries.loc[:, \"desc\"].str.extract(pattern)\n",
    "\n",
    "    # concatenation\n",
    "    plays_with_injuries = pd.concat([plays_with_injuries, injured_players], axis=1)\n",
    "    plays_with_injuries.rename(columns={0: \"injured_player\"}, inplace=True)\n",
    "    plays_with_injuries = plays_with_injuries.reset_index(drop=True)\n",
    "    \n",
    "    # merging\n",
    "    injuries = []\n",
    "    for (week, team), group_injury_df in injury_df.groupby(['week', 'team']):\n",
    "        group_play_df = plays_with_injuries[(plays_with_injuries['week'] == week) & ((plays_with_injuries['home_team'] == team) | (plays_with_injuries['away_team'] == team))]\n",
    "\n",
    "        group_injury_df = group_injury_df[group_injury_df.date >= group_play_df.date.max()]\n",
    "\n",
    "        group_injury_df[\"first_type\"] = group_injury_df['full_name'].apply(first_last_a)\n",
    "        group_injury_df[\"second_type\"] = group_injury_df['full_name'].apply(first_last_b)\n",
    "\n",
    "        x = pd.merge(group_play_df, group_injury_df, left_on=\"injured_player\", right_on=\"first_type\", how=\"inner\")\n",
    "        y = pd.merge(group_play_df, group_injury_df, left_on=\"injured_player\", right_on=\"second_type\", how=\"inner\")\n",
    "\n",
    "        injuries.append(pd.concat([x, y], axis = 0, ignore_index=True))\n",
    "\n",
    "    plays_with_injuries_and_injury_record = (pd.concat(injuries, axis=0, ignore_index=True)).drop(columns=[\"first_type\", \"second_type\"])\n",
    "    plays_with_injuries_and_injury_record = plays_with_injuries_and_injury_record.sort_values('play_id', ascending=False).drop_duplicates(subset=['week_x', 'full_name', \"team\"], keep='first')\n",
    "\n",
    "    return plays_with_injuries, plays_with_injuries_and_injury_record\n",
    "#returns (plays where injuries occurred, plays were injuries occurred and missed time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_cols_in_play_df(play_df, plays_with_injuries, plays_with_injuries_and_injury_record):\n",
    "    columns_to_check = ['play_id', 'game_id']\n",
    "    play_df[\"was_injured\"] = 0\n",
    "    play_df[\"missed_time\"] = 0\n",
    "    play_df.loc[play_df[columns_to_check].apply(tuple, 1).isin(plays_with_injuries[columns_to_check].apply(tuple, 1)), 'was_injured'] = 1\n",
    "    play_df.loc[play_df[columns_to_check].apply(tuple, 1).isin(plays_with_injuries_and_injury_record[columns_to_check].apply(tuple, 1)), 'missed_time'] = 1\n",
    "\n",
    "    return play_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_with_injuries_2024, plays_with_injuries_and_injury_record_2024 = merge_play_injury_dfs(play2024_df, injury2024_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of rows for each returned table\n",
    "print(\"Rows in plays_with_injuries_2024:\", len(plays_with_injuries_2024))\n",
    "print(\"Rows in plays_with_injuries_and_injury_record_2024:\", len(plays_with_injuries_and_injury_record_2024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df = populate_cols_in_play_df(play2024_df, plays_with_injuries_2024, plays_with_injuries_and_injury_record_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_needed = [\n",
    "    # \"play_id\",\n",
    "    # \"game_id\",\n",
    "    \"home_team\",\n",
    "    \"away_team\",\n",
    "    \"season_type\",\n",
    "    \"game_date\",\n",
    "    \"down\",\n",
    "    \"play_type\",\n",
    "    \"score_differential_post\",\n",
    "    # \"order_sequence\",\n",
    "    # \"time_of_day\",\n",
    "    \"stadium\",\n",
    "    \"weather\",\n",
    "    \"roof\",\n",
    "    \"surface\",\n",
    "    \"temp\",\n",
    "    \"wind\",\n",
    "    \"was_injured\",\n",
    "    \"missed_time\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = play2024_df[[*columns_needed]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(play2024_df['weather'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Assuming play2024_df is your DataFrame\n",
    "\n",
    "# Step 1: Extract and categorize weather condition\n",
    "def extract_weather_condition(description):\n",
    "    if pd.isna(description) or 'N/A' in description:\n",
    "        return 'Unknown'\n",
    "    \n",
    "    description = description.lower()\n",
    "    \n",
    "    # Expanded weather condition checks with more keywords\n",
    "    if any(word in description for word in ['sunny', 'mostly sunny', 'partly sunny', 'sun']):\n",
    "        return 'Sunny'\n",
    "    elif any(word in description for word in ['cloudy', 'mostly cloudy', 'partly cloudy', 'overcast']):\n",
    "        return 'Cloudy'\n",
    "    elif 'clear' in description:\n",
    "        return 'Clear'\n",
    "    elif 'rain' in description or 'rainy' in description:\n",
    "        return 'Rain'\n",
    "    elif 'fog' in description or 'foggy' in description:\n",
    "        return 'Foggy'\n",
    "    elif 'wind' in description or 'blustery' in description:\n",
    "        return 'Windy'\n",
    "    elif 'controlled climate' in description or 'indoors' in description:\n",
    "        return 'Controlled Climate'\n",
    "    elif 'fair' in description:\n",
    "        return 'Fair'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the improved categorization function\n",
    "play2024_df['weather_condition'] = play2024_df['weather'].apply(extract_weather_condition)\n",
    "\n",
    "# Step 2: Extract and categorize temperature\n",
    "def extract_temperature(description):\n",
    "    match = re.search(r'Temp: (\\d+)', description) if isinstance(description, str) else None\n",
    "    return int(match.group(1)) if match else np.nan\n",
    "\n",
    "play2024_df['temperature'] = play2024_df['weather'].apply(extract_temperature)\n",
    "\n",
    "def categorize_temperature(temp):\n",
    "    if pd.isna(temp):\n",
    "        return 'Unknown'\n",
    "    elif temp < 50:\n",
    "        return 'Cold'\n",
    "    elif 50 <= temp <= 75:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Hot'\n",
    "\n",
    "play2024_df['temperature_category'] = play2024_df['temperature'].apply(categorize_temperature)\n",
    "\n",
    "# Step 3: Extract and categorize humidity\n",
    "def extract_humidity(description):\n",
    "    match = re.search(r'Humidity: (\\d+)%', description) if isinstance(description, str) else None\n",
    "    return int(match.group(1)) if match else np.nan\n",
    "\n",
    "play2024_df['humidity'] = play2024_df['weather'].apply(extract_humidity)\n",
    "\n",
    "def categorize_humidity(humidity):\n",
    "    if pd.isna(humidity):\n",
    "        return 'Unknown'\n",
    "    elif humidity < 40:\n",
    "        return 'Low'\n",
    "    elif 40 <= humidity <= 70:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "play2024_df['humidity_category'] = play2024_df['humidity'].apply(categorize_humidity)\n",
    "\n",
    "# Step 4: Extract and categorize wind\n",
    "def extract_wind_speed(description):\n",
    "    match = re.search(r'Wind: [A-Za-z]* (\\d+)', description) if isinstance(description, str) else None\n",
    "    return int(match.group(1)) if match else np.nan\n",
    "\n",
    "play2024_df['wind_speed'] = play2024_df['weather'].apply(extract_wind_speed)\n",
    "\n",
    "def categorize_wind_speed(wind_speed):\n",
    "    if pd.isna(wind_speed):\n",
    "        return 'Unknown'\n",
    "    elif wind_speed < 5:\n",
    "        return 'Calm'\n",
    "    elif 5 <= wind_speed <= 15:\n",
    "        return 'Breezy'\n",
    "    else:\n",
    "        return 'Windy'\n",
    "\n",
    "play2024_df['wind_category'] = play2024_df['wind_speed'].apply(categorize_wind_speed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df[['weather', 'weather_condition', 'temperature', 'temperature_category', \n",
    "                   'humidity', 'humidity_category', 'wind_speed', 'wind_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df['weather_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping meta data columns\n",
    "modeling = final_df.drop(columns=[\"play_id\",\"game_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features and target\n",
    "numeric_features = [\"down\", \"score_differential_post\", \"temp\", \"wind\"]\n",
    "data = play2024_df[numeric_features + [\"was_injured\"]]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Analysis for Numeric Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a broader set of columns to improve context\n",
    "columns_needed = [\n",
    "    \"home_team\", \"away_team\", \"season_type\",\n",
    "    \"down\", \"play_type\", \"score_differential_post\", \n",
    "    \"stadium\", \"roof\", \"surface\", \"temp\", \"wind\", \"was_injured\", \"missed_time\",\n",
    "    \"weather_condition\", \"temperature_category\", \"humidity_category\", \"wind_category\"\n",
    "]\n",
    "\n",
    "# Select relevant columns from the DataFrame\n",
    "X = play2024_df[columns_needed].drop(columns=[\"was_injured\", \"missed_time\"])\n",
    "y = play2024_df['was_injured']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numeric_features = [\"down\", \"score_differential_post\", \"temp\", \"wind\"]\n",
    "categorical_features = [\"home_team\", \"away_team\", \"season_type\", \"play_type\", \"stadium\", \"roof\", \n",
    "                        \"surface\", \"weather_condition\", \"temperature_category\", \"humidity_category\", \"wind_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model with adjusted regularization\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, C=1.0, penalty='l2')\n",
    "\n",
    "# Apply SMOTE with a lower sampling strategy to avoid overcompensation\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# Create the pipeline with preprocessing, SMOTE, and the classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', smote),\n",
    "    ('classifier', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Adjust decision threshold\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get the probability of the positive class\n",
    "threshold = 0.5  # Adjust this threshold as needed (e.g., 0.35, 0.4, etc.)\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluate performance with the adjusted threshold\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from imblearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define columns to use\n",
    "columns_needed = [\n",
    "    \"home_team\", \"away_team\", \"season_type\",\n",
    "    \"down\", \"play_type\", \"score_differential_post\", \n",
    "    \"stadium\", \"roof\", \"surface\", \"temp\", \"wind\", \"was_injured\", \"missed_time\",\n",
    "    \"weather_condition\", \"temperature_category\", \"humidity_category\", \"wind_category\"\n",
    "]\n",
    "\n",
    "# Select relevant columns from the DataFrame\n",
    "data = play2024_df[columns_needed]\n",
    "\n",
    "# converting categorical data into numeric\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = label_encoder.fit_transform(data[col].astype(str))\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# Separate majority and minority classes\n",
    "injury_class = data[data['was_injured'] == 1]\n",
    "no_injury_class = data[data['was_injured'] == 0]\n",
    "\n",
    "# Bootstrapping the injury class to add more samples with replacement\n",
    "# bootstrap_injury = injury_class.sample(n=8000, replace=True, random_state=42)\n",
    "\n",
    "# Combine bootstrapped minority samples with majority class\n",
    "# augmented_data = pd.concat([no_injury_class, injury_class, bootstrap_injury])\n",
    "augmented_data = pd.concat([no_injury_class, injury_class])\n",
    "\n",
    "# Separate features and target\n",
    "X = augmented_data.drop(columns=[\"was_injured\", \"missed_time\"])\n",
    "y = augmented_data['was_injured']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numeric_features = [\"down\", \"score_differential_post\", \"temp\", \"wind\"]\n",
    "categorical_features = [\"home_team\", \"away_team\", \"season_type\", \"play_type\", \"stadium\", \"roof\", \n",
    "                        \"surface\", \"weather_condition\", \"temperature_category\", \"humidity_category\", \"wind_category\"]\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Balanced Random Forest model with SMOTE\n",
    "model = BalancedRandomForestClassifier(n_estimators=500, max_depth=3, random_state=42)\n",
    "\n",
    "# Hybrid sampling strategy with SMOTE after bootstrapping\n",
    "# smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "\n",
    "# Create the pipeline with preprocessing, SMOTE, and the classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', smote),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Adjust decision threshold\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get the probability of the positive class\n",
    "threshold = 0.5  # Adjust this threshold as needed (e.g., 0.35, 0.4, etc.)\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluate performance with the adjusted threshold\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix as a heatmap\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(cmap=\"Blues\")  # You can customize the color map if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
