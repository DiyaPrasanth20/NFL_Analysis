{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df = pd.read_csv(\"data/play_by_play_2024.csv\")\n",
    "injury2024_df = pd.read_csv(\"data/injuries_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_a(name):\n",
    "    parts = name.split(\" \", 1)\n",
    "    return f\"{parts[0][0]}.{parts[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_b(name):\n",
    "    parts = name.split(\" \", 1)\n",
    "    return f\"{parts[0][0:2]}.{parts[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def merge_play_injury_dfs(play_df, injury_df):\n",
    "\n",
    "    # preprocessing\n",
    "    injury_df[\"date\"] = pd.to_datetime(injury_df['date_modified'])\n",
    "    play_df[\"date\"] = pd.to_datetime(play_df['game_date'])\n",
    "    injury_df['date'] = injury_df['date'].dt.tz_localize(None)\n",
    "    play_df['date'] = play_df['date'].dt.tz_localize(None)\n",
    "\n",
    "    # filtering\n",
    "    plays_with_injuries = play_df[play_df['desc'].str.contains(\"was injured\", na=False)]\n",
    "    pattern = r'(\\w+\\.(?:\\w|-|\\.|\\')+(?: \\w+)*) was injured'\n",
    "    # Extract the injured player's name from the desc column\n",
    "    injured_players = plays_with_injuries.loc[:, \"desc\"].str.extract(pattern)\n",
    "\n",
    "    # concatenation\n",
    "    plays_with_injuries = pd.concat([plays_with_injuries, injured_players], axis=1)\n",
    "    plays_with_injuries.rename(columns={0: \"injured_player\"}, inplace=True)\n",
    "    plays_with_injuries = plays_with_injuries.reset_index(drop=True)\n",
    "    \n",
    "    # merging\n",
    "    injuries = []\n",
    "    for (week, team), group_injury_df in injury_df.groupby(['week', 'team']):\n",
    "        group_play_df = plays_with_injuries[(plays_with_injuries['week'] == week) & ((plays_with_injuries['home_team'] == team) | (plays_with_injuries['away_team'] == team))]\n",
    "\n",
    "        group_injury_df = group_injury_df[group_injury_df.date >= group_play_df.date.max()]\n",
    "\n",
    "        group_injury_df[\"first_type\"] = group_injury_df['full_name'].apply(first_last_a)\n",
    "        group_injury_df[\"second_type\"] = group_injury_df['full_name'].apply(first_last_b)\n",
    "\n",
    "        x = pd.merge(group_play_df, group_injury_df, left_on=\"injured_player\", right_on=\"first_type\", how=\"inner\")\n",
    "        y = pd.merge(group_play_df, group_injury_df, left_on=\"injured_player\", right_on=\"second_type\", how=\"inner\")\n",
    "\n",
    "        injuries.append(pd.concat([x, y], axis = 0, ignore_index=True))\n",
    "\n",
    "    plays_with_injuries_and_injury_record = (pd.concat(injuries, axis=0, ignore_index=True)).drop(columns=[\"first_type\", \"second_type\"])\n",
    "    plays_with_injuries_and_injury_record = plays_with_injuries_and_injury_record.sort_values('play_id', ascending=False).drop_duplicates(subset=['week_x', 'full_name', \"team\"], keep='first')\n",
    "\n",
    "    return plays_with_injuries, plays_with_injuries_and_injury_record\n",
    "#returns (plays where injuries occurred, plays were injuries occurred and missed time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_cols_in_play_df(play_df, plays_with_injuries, plays_with_injuries_and_injury_record):\n",
    "    columns_to_check = ['play_id', 'game_id']\n",
    "    play_df[\"was_injured\"] = 0\n",
    "    play_df[\"missed_time\"] = 0\n",
    "    play_df.loc[play_df[columns_to_check].apply(tuple, 1).isin(plays_with_injuries[columns_to_check].apply(tuple, 1)), 'was_injured'] = 1\n",
    "    play_df.loc[play_df[columns_to_check].apply(tuple, 1).isin(plays_with_injuries_and_injury_record[columns_to_check].apply(tuple, 1)), 'missed_time'] = 1\n",
    "\n",
    "    return play_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_with_injuries_2024, plays_with_injuries_and_injury_record_2024 = merge_play_injury_dfs(play2024_df, injury2024_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in plays_with_injuries_2024: 457\n",
      "Rows in plays_with_injuries_and_injury_record_2024: 185\n"
     ]
    }
   ],
   "source": [
    "# Print number of rows for each returned table\n",
    "print(\"Rows in plays_with_injuries_2024:\", len(plays_with_injuries_2024))\n",
    "print(\"Rows in plays_with_injuries_and_injury_record_2024:\", len(plays_with_injuries_and_injury_record_2024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26345\n",
      "375\n",
      "['play_id', 'game_id', 'old_game_id', 'home_team', 'away_team', 'season_type', 'week', 'posteam', 'posteam_type', 'defteam', 'side_of_field', 'yardline_100', 'game_date', 'quarter_seconds_remaining', 'half_seconds_remaining', 'game_seconds_remaining', 'game_half', 'quarter_end', 'drive', 'sp', 'qtr', 'down', 'goal_to_go', 'time', 'yrdln', 'ydstogo', 'ydsnet', 'desc', 'play_type', 'yards_gained', 'shotgun', 'no_huddle', 'qb_dropback', 'qb_kneel', 'qb_spike', 'qb_scramble', 'pass_length', 'pass_location', 'air_yards', 'yards_after_catch', 'run_location', 'run_gap', 'field_goal_result', 'kick_distance', 'extra_point_result', 'two_point_conv_result', 'home_timeouts_remaining', 'away_timeouts_remaining', 'timeout', 'timeout_team', 'td_team', 'td_player_name', 'td_player_id', 'posteam_timeouts_remaining', 'defteam_timeouts_remaining', 'total_home_score', 'total_away_score', 'posteam_score', 'defteam_score', 'score_differential', 'posteam_score_post', 'defteam_score_post', 'score_differential_post', 'no_score_prob', 'opp_fg_prob', 'opp_safety_prob', 'opp_td_prob', 'fg_prob', 'safety_prob', 'td_prob', 'extra_point_prob', 'two_point_conversion_prob', 'ep', 'epa', 'total_home_epa', 'total_away_epa', 'total_home_rush_epa', 'total_away_rush_epa', 'total_home_pass_epa', 'total_away_pass_epa', 'air_epa', 'yac_epa', 'comp_air_epa', 'comp_yac_epa', 'total_home_comp_air_epa', 'total_away_comp_air_epa', 'total_home_comp_yac_epa', 'total_away_comp_yac_epa', 'total_home_raw_air_epa', 'total_away_raw_air_epa', 'total_home_raw_yac_epa', 'total_away_raw_yac_epa', 'wp', 'def_wp', 'home_wp', 'away_wp', 'wpa', 'vegas_wpa', 'vegas_home_wpa', 'home_wp_post', 'away_wp_post', 'vegas_wp', 'vegas_home_wp', 'total_home_rush_wpa', 'total_away_rush_wpa', 'total_home_pass_wpa', 'total_away_pass_wpa', 'air_wpa', 'yac_wpa', 'comp_air_wpa', 'comp_yac_wpa', 'total_home_comp_air_wpa', 'total_away_comp_air_wpa', 'total_home_comp_yac_wpa', 'total_away_comp_yac_wpa', 'total_home_raw_air_wpa', 'total_away_raw_air_wpa', 'total_home_raw_yac_wpa', 'total_away_raw_yac_wpa', 'punt_blocked', 'first_down_rush', 'first_down_pass', 'first_down_penalty', 'third_down_converted', 'third_down_failed', 'fourth_down_converted', 'fourth_down_failed', 'incomplete_pass', 'touchback', 'interception', 'punt_inside_twenty', 'punt_in_endzone', 'punt_out_of_bounds', 'punt_downed', 'punt_fair_catch', 'kickoff_inside_twenty', 'kickoff_in_endzone', 'kickoff_out_of_bounds', 'kickoff_downed', 'kickoff_fair_catch', 'fumble_forced', 'fumble_not_forced', 'fumble_out_of_bounds', 'solo_tackle', 'safety', 'penalty', 'tackled_for_loss', 'fumble_lost', 'own_kickoff_recovery', 'own_kickoff_recovery_td', 'qb_hit', 'rush_attempt', 'pass_attempt', 'sack', 'touchdown', 'pass_touchdown', 'rush_touchdown', 'return_touchdown', 'extra_point_attempt', 'two_point_attempt', 'field_goal_attempt', 'kickoff_attempt', 'punt_attempt', 'fumble', 'complete_pass', 'assist_tackle', 'lateral_reception', 'lateral_rush', 'lateral_return', 'lateral_recovery', 'passer_player_id', 'passer_player_name', 'passing_yards', 'receiver_player_id', 'receiver_player_name', 'receiving_yards', 'rusher_player_id', 'rusher_player_name', 'rushing_yards', 'lateral_receiver_player_id', 'lateral_receiver_player_name', 'lateral_receiving_yards', 'lateral_rusher_player_id', 'lateral_rusher_player_name', 'lateral_rushing_yards', 'lateral_sack_player_id', 'lateral_sack_player_name', 'interception_player_id', 'interception_player_name', 'lateral_interception_player_id', 'lateral_interception_player_name', 'punt_returner_player_id', 'punt_returner_player_name', 'lateral_punt_returner_player_id', 'lateral_punt_returner_player_name', 'kickoff_returner_player_name', 'kickoff_returner_player_id', 'lateral_kickoff_returner_player_id', 'lateral_kickoff_returner_player_name', 'punter_player_id', 'punter_player_name', 'kicker_player_name', 'kicker_player_id', 'own_kickoff_recovery_player_id', 'own_kickoff_recovery_player_name', 'blocked_player_id', 'blocked_player_name', 'tackle_for_loss_1_player_id', 'tackle_for_loss_1_player_name', 'tackle_for_loss_2_player_id', 'tackle_for_loss_2_player_name', 'qb_hit_1_player_id', 'qb_hit_1_player_name', 'qb_hit_2_player_id', 'qb_hit_2_player_name', 'forced_fumble_player_1_team', 'forced_fumble_player_1_player_id', 'forced_fumble_player_1_player_name', 'forced_fumble_player_2_team', 'forced_fumble_player_2_player_id', 'forced_fumble_player_2_player_name', 'solo_tackle_1_team', 'solo_tackle_2_team', 'solo_tackle_1_player_id', 'solo_tackle_2_player_id', 'solo_tackle_1_player_name', 'solo_tackle_2_player_name', 'assist_tackle_1_player_id', 'assist_tackle_1_player_name', 'assist_tackle_1_team', 'assist_tackle_2_player_id', 'assist_tackle_2_player_name', 'assist_tackle_2_team', 'assist_tackle_3_player_id', 'assist_tackle_3_player_name', 'assist_tackle_3_team', 'assist_tackle_4_player_id', 'assist_tackle_4_player_name', 'assist_tackle_4_team', 'tackle_with_assist', 'tackle_with_assist_1_player_id', 'tackle_with_assist_1_player_name', 'tackle_with_assist_1_team', 'tackle_with_assist_2_player_id', 'tackle_with_assist_2_player_name', 'tackle_with_assist_2_team', 'pass_defense_1_player_id', 'pass_defense_1_player_name', 'pass_defense_2_player_id', 'pass_defense_2_player_name', 'fumbled_1_team', 'fumbled_1_player_id', 'fumbled_1_player_name', 'fumbled_2_player_id', 'fumbled_2_player_name', 'fumbled_2_team', 'fumble_recovery_1_team', 'fumble_recovery_1_yards', 'fumble_recovery_1_player_id', 'fumble_recovery_1_player_name', 'fumble_recovery_2_team', 'fumble_recovery_2_yards', 'fumble_recovery_2_player_id', 'fumble_recovery_2_player_name', 'sack_player_id', 'sack_player_name', 'half_sack_1_player_id', 'half_sack_1_player_name', 'half_sack_2_player_id', 'half_sack_2_player_name', 'return_team', 'return_yards', 'penalty_team', 'penalty_player_id', 'penalty_player_name', 'penalty_yards', 'replay_or_challenge', 'replay_or_challenge_result', 'penalty_type', 'defensive_two_point_attempt', 'defensive_two_point_conv', 'defensive_extra_point_attempt', 'defensive_extra_point_conv', 'safety_player_name', 'safety_player_id', 'season', 'cp', 'cpoe', 'series', 'series_success', 'series_result', 'order_sequence', 'start_time', 'time_of_day', 'stadium', 'weather', 'nfl_api_id', 'play_clock', 'play_deleted', 'play_type_nfl', 'special_teams_play', 'st_play_type', 'end_clock_time', 'end_yard_line', 'fixed_drive', 'fixed_drive_result', 'drive_real_start_time', 'drive_play_count', 'drive_time_of_possession', 'drive_first_downs', 'drive_inside20', 'drive_ended_with_score', 'drive_quarter_start', 'drive_quarter_end', 'drive_yards_penalized', 'drive_start_transition', 'drive_end_transition', 'drive_game_clock_start', 'drive_game_clock_end', 'drive_start_yard_line', 'drive_end_yard_line', 'drive_play_id_started', 'drive_play_id_ended', 'away_score', 'home_score', 'location', 'result', 'total', 'spread_line', 'total_line', 'div_game', 'roof', 'surface', 'temp', 'wind', 'home_coach', 'away_coach', 'stadium_id', 'game_stadium', 'aborted_play', 'success', 'passer', 'passer_jersey_number', 'rusher', 'rusher_jersey_number', 'receiver', 'receiver_jersey_number', 'pass', 'rush', 'first_down', 'special', 'play', 'passer_id', 'rusher_id', 'receiver_id', 'name', 'jersey_number', 'id', 'fantasy_player_name', 'fantasy_player_id', 'fantasy', 'fantasy_id', 'out_of_bounds', 'home_opening_kickoff', 'qb_epa', 'xyac_epa', 'xyac_mean_yardage', 'xyac_median_yardage', 'xyac_success', 'xyac_fd', 'xpass', 'pass_oe', 'date', 'was_injured', 'missed_time']\n"
     ]
    }
   ],
   "source": [
    "play2024_df = populate_cols_in_play_df(play2024_df, plays_with_injuries_2024, plays_with_injuries_and_injury_record_2024)\n",
    "print(len(play2024_df))\n",
    "print(len(play2024_df.columns.tolist()))\n",
    "print(play2024_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns from X\n",
    "X = play2024_df.drop(columns=[\n",
    "    'was_injured', 'missed_time', 'play_id', 'game_id', 'game_date', 'desc', \n",
    "    'td_player_name', 'passer_player_name', 'rusher_player_name', 'receiver_player_name', \n",
    "    'nfl_api_id', 'fantasy_player_name', 'fantasy_player_id', 'passer_jersey_number', \n",
    "    'rusher_jersey_number', 'receiver_jersey_number', 'jersey_number'\n",
    "])\n",
    "\n",
    "# Drop all columns with datetime64 data type from X\n",
    "X = X.select_dtypes(exclude=['datetime64'])\n",
    "\n",
    "# Define target variables\n",
    "y_injury = play2024_df['was_injured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was_injured\n",
      "0    0.982653\n",
      "1    0.017347\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(play2024_df['was_injured'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 25888, 1: 457})\n",
      "Undersampled class distribution: Counter({0: 411, 1: 411})\n",
      "\n",
      "*** Using Undersampling ***\n",
      "\n",
      "=== Random Forest (Balanced) ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.54      0.70      2589\n",
      "           1       0.02      0.63      0.05        46\n",
      "\n",
      "    accuracy                           0.54      2635\n",
      "   macro avg       0.51      0.58      0.37      2635\n",
      "weighted avg       0.97      0.54      0.69      2635\n",
      "\n",
      "ROC-AUC Score: 0.6101\n",
      "Confusion Matrix:\n",
      "[[1395 1194]\n",
      " [  17   29]]\n",
      "Time taken: 0.53 seconds\n",
      "\n",
      "=== LightGBM ===\n",
      "[LightGBM] [Info] Number of positive: 411, number of negative: 411\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17839\n",
      "[LightGBM] [Info] Number of data points in the train set: 822, number of used features: 540\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.57      0.72      2589\n",
      "           1       0.02      0.59      0.05        46\n",
      "\n",
      "    accuracy                           0.57      2635\n",
      "   macro avg       0.51      0.58      0.38      2635\n",
      "weighted avg       0.97      0.57      0.71      2635\n",
      "\n",
      "ROC-AUC Score: 0.6094\n",
      "Confusion Matrix:\n",
      "[[1470 1119]\n",
      " [  19   27]]\n",
      "Time taken: 0.55 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "\n",
    "# Step 1: Handle Categorical and Missing Values\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_transformer = SimpleImputer(strategy='mean')  # Replace NaN with column mean\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace NaN with mode\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply Preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_injury, stratify=y_injury, random_state=42, test_size=0.1\n",
    ")\n",
    "\n",
    "# Step 3: Check Class Distribution\n",
    "print(\"Original class distribution:\", Counter(y_injury))\n",
    "\n",
    "# 2. Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "print(\"Undersampled class distribution:\", Counter(y_train_undersampled))\n",
    "\n",
    "# Step 4: Model Training and Evaluation\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Models to Evaluate\n",
    "models = [\n",
    "    (\"Random Forest (Balanced)\", RandomForestClassifier(class_weight=\"balanced\", random_state=42)),\n",
    "    (\"LightGBM\", lgb.LGBMClassifier(is_unbalance=True, random_state=42)),\n",
    "]\n",
    "\n",
    "# Evaluate Models with Undersampling\n",
    "print(\"\\n*** Using Undersampling ***\")\n",
    "for model_name, model in models:\n",
    "    start = time.time()\n",
    "    train_and_evaluate_model(model, model_name, X_train_undersampled, y_train_undersampled, X_test, y_test)\n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total explained variance by PCA: 1.0000\n",
      "Undersampled class distribution: Counter({0: 366, 1: 366})\n",
      "\n",
      "*** Using PCA and Undersampling ***\n",
      "\n",
      "=== Random Forest (Balanced) ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.56      0.72      5178\n",
      "           1       0.02      0.63      0.05        91\n",
      "\n",
      "    accuracy                           0.56      5269\n",
      "   macro avg       0.51      0.59      0.38      5269\n",
      "weighted avg       0.97      0.56      0.70      5269\n",
      "\n",
      "ROC-AUC Score: 0.6409\n",
      "Confusion Matrix:\n",
      "[[2905 2273]\n",
      " [  34   57]]\n",
      "Time taken: 0.26 seconds\n",
      "\n",
      "=== LightGBM ===\n",
      "[LightGBM] [Info] Number of positive: 366, number of negative: 366\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12213\n",
      "[LightGBM] [Info] Number of data points in the train set: 732, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.57      0.72      5178\n",
      "           1       0.02      0.59      0.05        91\n",
      "\n",
      "    accuracy                           0.57      5269\n",
      "   macro avg       0.51      0.58      0.38      5269\n",
      "weighted avg       0.97      0.57      0.71      5269\n",
      "\n",
      "ROC-AUC Score: 0.6407\n",
      "Confusion Matrix:\n",
      "[[2951 2227]\n",
      " [  37   54]]\n",
      "Time taken: 0.33 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "\n",
    "# Step 1: Handle Categorical and Missing Values\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_transformer = SimpleImputer(strategy='mean')  # Replace NaN with column mean\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace NaN with mode\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply Preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert sparse matrix to dense\n",
    "X_dense = X_processed.toarray()\n",
    "\n",
    "# Step 2: PCA Dimensionality Reduction\n",
    "pca = PCA(n_components=50)  # Retain 50 principal components (adjust as needed)\n",
    "X_pca = pca.fit_transform(X_dense)\n",
    "\n",
    "# Calculate and display explained variance\n",
    "total_variance_explained = sum(pca.explained_variance_ratio_)\n",
    "print(f\"Total explained variance by PCA: {total_variance_explained:.4f}\")\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y_injury, stratify=y_injury, random_state=42, test_size=0.2\n",
    ")\n",
    "\n",
    "# Step 4: Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "print(\"Undersampled class distribution:\", Counter(y_train_undersampled))\n",
    "\n",
    "# Step 5: Model Training and Evaluation\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Models to Evaluate\n",
    "models = [\n",
    "    (\"Random Forest (Balanced)\", RandomForestClassifier(class_weight=\"balanced\", random_state=42)),\n",
    "    (\"LightGBM\", lgb.LGBMClassifier(is_unbalance=True, random_state=42)),\n",
    "]\n",
    "\n",
    "# Evaluate Models with PCA and Undersampling\n",
    "print(\"\\n*** Using PCA and Undersampling ***\")\n",
    "for model_name, model in models:\n",
    "    start = time.time()\n",
    "    train_and_evaluate_model(model, model_name, X_train_undersampled, y_train_undersampled, X_test, y_test)\n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end - start:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs373",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
