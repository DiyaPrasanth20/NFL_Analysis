{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df = pd.read_csv(\"data/play_by_play_2024.csv\")\n",
    "injury2024_df = pd.read_csv(\"data/injuries_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_a(name):\n",
    "    parts = name.split(\" \", 1)\n",
    "    return f\"{parts[0][0]}.{parts[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_b(name):\n",
    "    parts = name.split(\" \", 1)\n",
    "    return f\"{parts[0][0:2]}.{parts[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def merge_play_injury_dfs(play_df, injury_df):\n",
    "\n",
    "    # preprocessing\n",
    "    injury_df[\"date\"] = pd.to_datetime(injury_df['date_modified'])\n",
    "    play_df[\"date\"] = pd.to_datetime(play_df['game_date'])\n",
    "    injury_df['date'] = injury_df['date'].dt.tz_localize(None)\n",
    "    play_df['date'] = play_df['date'].dt.tz_localize(None)\n",
    "\n",
    "    # filtering\n",
    "    plays_with_injuries = play_df[play_df['desc'].str.contains(\"was injured\", na=False)]\n",
    "    pattern = r'(\\w+\\.(?:\\w|-|\\.|\\')+(?: \\w+)*) was injured'\n",
    "    # Extract the injured player's name from the desc column\n",
    "    injured_players = plays_with_injuries.loc[:, \"desc\"].str.extract(pattern)\n",
    "\n",
    "    # concatenation\n",
    "    plays_with_injuries = pd.concat([plays_with_injuries, injured_players], axis=1)\n",
    "    plays_with_injuries.rename(columns={0: \"injured_player\"}, inplace=True)\n",
    "    plays_with_injuries = plays_with_injuries.reset_index(drop=True)\n",
    "    \n",
    "    # merging\n",
    "    injuries = []\n",
    "    for (week, team), group_injury_df in injury_df.groupby(['week', 'team']):\n",
    "        group_play_df = plays_with_injuries[(plays_with_injuries['week'] == week) & ((plays_with_injuries['home_team'] == team) | (plays_with_injuries['away_team'] == team))]\n",
    "\n",
    "        group_injury_df = group_injury_df[group_injury_df.date >= group_play_df.date.max()]\n",
    "\n",
    "        group_injury_df[\"first_type\"] = group_injury_df['full_name'].apply(first_last_a)\n",
    "        group_injury_df[\"second_type\"] = group_injury_df['full_name'].apply(first_last_b)\n",
    "\n",
    "        x = pd.merge(group_play_df, group_injury_df, left_on=\"injured_player\", right_on=\"first_type\", how=\"inner\")\n",
    "        y = pd.merge(group_play_df, group_injury_df, left_on=\"injured_player\", right_on=\"second_type\", how=\"inner\")\n",
    "\n",
    "        injuries.append(pd.concat([x, y], axis = 0, ignore_index=True))\n",
    "\n",
    "    plays_with_injuries_and_injury_record = (pd.concat(injuries, axis=0, ignore_index=True)).drop(columns=[\"first_type\", \"second_type\"])\n",
    "    plays_with_injuries_and_injury_record = plays_with_injuries_and_injury_record.sort_values('play_id', ascending=False).drop_duplicates(subset=['week_x', 'full_name', \"team\"], keep='first')\n",
    "\n",
    "    return plays_with_injuries, plays_with_injuries_and_injury_record\n",
    "#returns (plays where injuries occurred, plays were injuries occurred and missed time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_cols_in_play_df(play_df, plays_with_injuries, plays_with_injuries_and_injury_record):\n",
    "    columns_to_check = ['play_id', 'game_id']\n",
    "    play_df[\"was_injured\"] = 0\n",
    "    play_df[\"missed_time\"] = 0\n",
    "    play_df.loc[play_df[columns_to_check].apply(tuple, 1).isin(plays_with_injuries[columns_to_check].apply(tuple, 1)), 'was_injured'] = 1\n",
    "    play_df.loc[play_df[columns_to_check].apply(tuple, 1).isin(plays_with_injuries_and_injury_record[columns_to_check].apply(tuple, 1)), 'missed_time'] = 1\n",
    "\n",
    "    return play_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_with_injuries_2024, plays_with_injuries_and_injury_record_2024 = merge_play_injury_dfs(play2024_df, injury2024_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of rows for each returned table\n",
    "print(\"Rows in plays_with_injuries_2024:\", len(plays_with_injuries_2024))\n",
    "print(\"Rows in plays_with_injuries_and_injury_record_2024:\", len(plays_with_injuries_and_injury_record_2024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play2024_df = populate_cols_in_play_df(play2024_df, plays_with_injuries_2024, plays_with_injuries_and_injury_record_2024)\n",
    "print(len(play2024_df))\n",
    "print(len(play2024_df.columns.tolist()))\n",
    "print(play2024_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in X_train; filling missing values.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Apply SMOTE to the training sets to balance the classes\u001b[39;00m\n\u001b[1;32m     46\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m X_train_injury_balanced, y_train_injury_balanced \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_injury\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m X_train_severe_balanced, y_train_severe_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train_severe)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Compute class weights based on the training labels\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/imblearn/base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m check_classification_targets(y)\n\u001b[1;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/imblearn/base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/sklearn/utils/validation.py:958\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 958\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs373/lib/python3.11/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns from X\n",
    "X = play2024_df.drop(columns=[\n",
    "    'was_injured', 'missed_time', 'play_id', 'game_id', 'game_date', 'desc', \n",
    "    'td_player_name', 'passer_player_name', 'rusher_player_name', 'receiver_player_name', \n",
    "    'nfl_api_id', 'fantasy_player_name', 'fantasy_player_id', 'passer_jersey_number', \n",
    "    'rusher_jersey_number', 'receiver_jersey_number', 'jersey_number'\n",
    "])\n",
    "\n",
    "# Drop all columns with datetime64 data type from X\n",
    "X = X.select_dtypes(exclude=['datetime64'])\n",
    "\n",
    "# Define target variables\n",
    "y_injury = play2024_df['was_injured']\n",
    "y_severe_injury = play2024_df['missed_time']\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill missing values for numerical columns with the mean\n",
    "X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].mean())\n",
    "\n",
    "# Fill missing values for categorical columns with 'Unknown'\n",
    "X[categorical_cols] = X[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[column] = le.fit_transform(X[column].astype(str))\n",
    "    label_encoders[column] = le  # Store encoders for future use\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train_injury, y_test_injury = train_test_split(X, y_injury, test_size=0.3, random_state=42)\n",
    "_, _, y_train_severe, y_test_severe = train_test_split(X, y_severe_injury, test_size=0.3, random_state=42)\n",
    "\n",
    "# Re-check and fill any remaining NaN values in X_train\n",
    "if X_train.isnull().any().any():\n",
    "    print(\"NaN values found in X_train; filling missing values.\")\n",
    "    # Fill any remaining NaNs in numerical columns with the mean\n",
    "    X_train[numerical_cols] = X_train[numerical_cols].fillna(X_train[numerical_cols].mean())\n",
    "    # Fill any remaining NaNs in categorical columns with 'Unknown'\n",
    "    X_train[categorical_cols] = X_train[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Apply SMOTE to the training sets to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_injury_balanced, y_train_injury_balanced = smote.fit_resample(X_train, y_train_injury)\n",
    "X_train_severe_balanced, y_train_severe_balanced = smote.fit_resample(X_train, y_train_severe)\n",
    "\n",
    "# Compute class weights based on the training labels\n",
    "class_weights_injury = class_weight.compute_sample_weight('balanced', y_train_injury_balanced)\n",
    "class_weights_severe = class_weight.compute_sample_weight('balanced', y_train_severe_balanced)\n",
    "\n",
    "# Train the Random Forest for predicting injury\n",
    "rf_injury = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_injury.fit(X_train_injury_balanced, y_train_injury_balanced, sample_weight=class_weights_injury)\n",
    "\n",
    "# Train the Random Forest for predicting severe injury\n",
    "rf_severe = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_severe.fit(X_train_severe_balanced, y_train_severe_balanced, sample_weight=class_weights_severe)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_injury = rf_injury.predict(X_test)\n",
    "y_pred_severe = rf_severe.predict(X_test)\n",
    "\n",
    "# Print classification report for both models\n",
    "print(\"Injury Prediction Report\")\n",
    "print(classification_report(y_test_injury, y_pred_injury, target_names=['No Injury', 'Injury']))\n",
    "\n",
    "print(\"\\nSevere Injury Prediction Report\")\n",
    "print(classification_report(y_test_severe, y_pred_severe, target_names=['No Missed Time', 'Missed Time']))\n",
    "\n",
    "# Feature importance\n",
    "injury_importances = pd.Series(rf_injury.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "severe_importances = pd.Series(rf_severe.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Display the top important features for both models\n",
    "print(\"\\nTop Features for Injury Prediction:\")\n",
    "print(injury_importances.head(10))\n",
    "\n",
    "print(\"\\nTop Features for Severe Injury Prediction:\")\n",
    "print(severe_importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = play2024_df.drop(columns=[\n",
    "    'was_injured', 'missed_time', 'play_id', 'game_id', 'game_date', 'desc', \n",
    "    'td_player_name', 'passer_player_name', 'rusher_player_name', 'receiver_player_name', \n",
    "    'nfl_api_id', 'fantasy_player_name', 'fantasy_player_id', 'passer_jersey_number', 'rusher_jersey_number',\n",
    "    'receiver_jersey_number', 'jersey_number'\n",
    "])\n",
    "\n",
    "# Drop all columns with datetime64 data type from X\n",
    "X = X.select_dtypes(exclude=['datetime64'])\n",
    "\n",
    "y_injury = play2024_df['was_injured']\n",
    "y_severe_injury = play2024_df['missed_time']\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill missing values for numerical columns with the mean\n",
    "X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].mean())\n",
    "\n",
    "# Fill missing values for categorical columns with 'Unknown'\n",
    "X[categorical_cols] = X[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[column] = le.fit_transform(X[column].astype(str))\n",
    "    label_encoders[column] = le  # Store encoders for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train_injury, y_test_injury = train_test_split(X, y_injury, test_size=0.3, random_state=42)\n",
    "_, _, y_train_severe, y_test_severe = train_test_split(X, y_severe_injury, test_size=0.3, random_state=42)\n",
    "\n",
    "# Compute class weights based on the training labels, not the entire dataset\n",
    "class_weights_injury = class_weight.compute_sample_weight('balanced', y_train_injury)\n",
    "class_weights_severe = class_weight.compute_sample_weight('balanced', y_train_severe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest for predicting injury\n",
    "rf_injury = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_injury.fit(X_train, y_train_injury, sample_weight=class_weights_injury)\n",
    "\n",
    "\n",
    "# Train the Random Forest for predicting severe injury\n",
    "rf_severe = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_severe.fit(X_train, y_train_severe, sample_weight=class_weights_severe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_injury = rf_injury.predict(X_test)\n",
    "y_pred_severe = rf_severe.predict(X_test)\n",
    "\n",
    "# Print classification report for both models\n",
    "print(\"Injury Prediction Report\")\n",
    "print(classification_report(y_test_injury, y_pred_injury, target_names=['No Injury', 'Injury']))\n",
    "\n",
    "print(\"\\nSevere Injury Prediction Report\")\n",
    "print(classification_report(y_test_severe, y_pred_severe, target_names=['No Missed Time', 'Missed Time']))\n",
    "\n",
    "# Feature importance\n",
    "injury_importances = pd.Series(rf_injury.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "severe_importances = pd.Series(rf_severe.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Display the top important features for both models\n",
    "print(\"\\nTop Features for Injury Prediction:\")\n",
    "print(injury_importances.head(10))\n",
    "\n",
    "print(\"\\nTop Features for Severe Injury Prediction:\")\n",
    "print(severe_importances.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs373",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
